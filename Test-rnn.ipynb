{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet.ndarray as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context= mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600901\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/nlp/nietzsche.txt\") as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zeros for padding\n",
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600901\n"
     ]
    }
   ],
   "source": [
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200297, 3)\n"
     ]
    }
   ],
   "source": [
    "col_concat = np.array([x1,x2,x3])\n",
    "t_col_concat = col_concat.T\n",
    "print(t_col_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_nd = mx.nd.array(x1)\n",
    "x2_nd = mx.nd.array(x2)\n",
    "x3_nd = mx.nd.array(x3)\n",
    "sample_input = mx.nd.array([ [x1[0],x2[0],x3[0]] ,[x1[1],x2[1],x3[1] ] ])\n",
    "\n",
    "simple_train_data = mx.nd.array(t_col_concat)\n",
    "simple_label_data = mx.nd.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def get_batch(source,label_data, i,batch_size=32):\n",
    "    bb_size = min(batch_size, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + bb_size]\n",
    "    target = label_data[i: i + bb_size]\n",
    "    #print(target.shape)\n",
    "    return data, target.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "test_bat,test_target = get_batch(simple_train_data,simple_label_data,5,batch_size)\n",
    "print(test_bat.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batchify(data, batch_size):\n",
    "#     \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "#     nbatch = data.shape[0] // batch_size\n",
    "#     data = data[:nbatch * batch_size]\n",
    "#     data = data.reshape((batch_size, nbatch)).T\n",
    "#     return data\n",
    "\n",
    "# train_data = batchify(idx_data, args_batch_size).as_in_context(context)\n",
    "# train_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block, nn\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "class UnRolledRNN_Model(Block):\n",
    "    def __init__(self,vocab_size, num_embed, num_hidden,**kwargs):\n",
    "        super(UnRolledRNN_Model, self).__init__(**kwargs)\n",
    "        self.num_embed = num_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # use name_scope to give child Blocks appropriate names.\n",
    "        # It also allows sharing Parameters between Blocks recursively.\n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "            self.dense1 = nn.Dense(num_hidden,activation='relu',flatten=True)\n",
    "            self.dense2 = nn.Dense(num_hidden,activation='relu',flatten=True)\n",
    "            self.dense3 = nn.Dense(vocab_size,flatten=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emd = self.encoder(inputs)\n",
    "        #print(emd.shape)\n",
    "        #since the input is shape(batch_size,input(3 characters))\n",
    "        # we need to extract 0th,1st,2nd character from each batch\n",
    "        chararcter1 = emd[:,0,:]\n",
    "        chararcter2 = emd[:,1,:]\n",
    "        chararcter3 = emd[:,2,:]\n",
    "        c1_hidden = self.dense1(chararcter1) # green arrow in diagram for character 1\n",
    "        c2_hidden = self.dense1(chararcter2) # green arrow in diagram for character 2\n",
    "        c3_hidden = self.dense1(chararcter3) # green arrow in diagram for character 3\n",
    "        c1_hidden_2 = self.dense2(c1_hidden)  # yellow arrow in diagram\n",
    "        addition_result = F.add(c2_hidden,c1_hidden_2) # Total c1 + c2\n",
    "        addition_hidden = self.dense2(addition_result) # the yellow arrow\n",
    "        addition_result_2 = F.add(addition_hidden,c3_hidden) # Total c1 + c2\n",
    "        final_output = self.dense3(addition_result_2)      \n",
    "        return final_output\n",
    "    \n",
    "vocab_size = len(chars)+1 # the vocabsize\n",
    "num_embed = 30\n",
    "num_hidden = 256\n",
    "simple_model = UnRolledRNN_Model(vocab_size, num_embed, num_hidden)\n",
    "simple_model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(simple_model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "#sample input shape is of size (2x3)\n",
    "#output = simple_model(sample_input)\n",
    "#sample out shape should be(3*87). 87 is our vocab size\n",
    "#print('the output shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "filename_unrolled_rnn = \"checkpoints/rnn_gluon.params\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnRolledRNNtrain(train_data,label_data,batch_size=32,epochs=10):\n",
    "    epochs = epochs\n",
    "    smoothing_constant = .01\n",
    "    for e in range(epochs):\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, batch_size)):\n",
    "            data, target = get_batch(train_data,label_data, i,batch_size)\n",
    "            data = data.as_in_context(context)\n",
    "            target = target.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                output = simple_model(data)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if ibatch == 128:\n",
    "                curr_loss = mx.nd.mean(L).asscalar()\n",
    "                moving_loss = 0\n",
    "                moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                           else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "                print(\"Epoch %s. Loss: %s, moving_loss %s\" % (e,curr_loss,moving_loss))   \n",
    "    simple_model.save_params(filename_unrolled_rnn)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.57893, moving_loss 0.0257892537117\n",
      "Epoch 1. Loss: 2.11341, moving_loss 0.0211340808868\n",
      "Epoch 2. Loss: 2.16297, moving_loss 0.0216296601295\n",
      "Epoch 3. Loss: 2.11876, moving_loss 0.0211876249313\n",
      "Epoch 4. Loss: 2.05449, moving_loss 0.0205448579788\n",
      "Epoch 5. Loss: 2.05682, moving_loss 0.0205682468414\n",
      "Epoch 6. Loss: 2.06466, moving_loss 0.020646572113\n",
      "Epoch 7. Loss: 2.06384, moving_loss 0.02063839674\n",
      "Epoch 8. Loss: 2.04167, moving_loss 0.0204166507721\n",
      "Epoch 9. Loss: 2.03729, moving_loss 0.0203729319572\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "UnRolledRNNtrain(simple_train_data,simple_label_data,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.load_params(filename_unrolled_rnn, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_string):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    sample_input = mx.nd.array([[ idx[0],idx[1],idx[2] ]],ctx=context)\n",
    "    output = simple_model(sample_input)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    return index.asnumpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted answer is  e\n"
     ]
    }
   ],
   "source": [
    "begin_char = 'lov'\n",
    "answer = evaluate(begin_char)\n",
    "print('the predicted answer is ',indices_char[answer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character RNN using gluon/lstm api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GluonRNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, **kwargs):\n",
    "        super(GluonRNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "               \n",
    "            if mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "    #define the forward pass of the neural network\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        #print('output forward',output.shape)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    #Initial state of netork\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'lstm'\n",
    "vocab_size = len(chars)+1 # number of characters in vocab_size\n",
    "embedsize = 100\n",
    "hididen_units = 50\n",
    "number_layers = 2\n",
    "clip = 0.2\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "seq_length = 100 # sequence length\n",
    "dropout = 0.4\n",
    "log_interval = 500\n",
    "rnn_save = 'checkpoints/gluonlstm'\n",
    "\n",
    "\n",
    "\n",
    "#RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = GluonRNNModel(mode, vocab_size, embedsize, hididen_units,\n",
    "                       number_layers, dropout)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# prepares rnn batches\n",
    "# The batch will be of shape is (num_example * batch_size) because of RNN uses sequences\n",
    "# In feedforward the shape is (batch_size, num_example)\n",
    "def rnn_batch(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "idx_nd = mx.nd.array(idx)\n",
    "# convert the idex of characters\n",
    "train_data_rnn_gluon = rnn_batch(idx_nd, batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i,seq):\n",
    "    seq_len = min(seq, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGluonRNN(epochs,train_data,seq=seq_length):\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, seq_length)):\n",
    "            data, target = get_batch(train_data, i,seq)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, clip * seq_length * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L /  seq_length / batch_size / log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f',epoch + 1, ibatch, cur_L)\n",
    "                total_L = 0.0\n",
    "        model.save_params(rnn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train data shape is (18778, 32)\n"
     ]
    }
   ],
   "source": [
    "print('the train data shape is',train_data_rnn_gluon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGluonRNN(epochs,train_data_rnn_gluon,seq=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_params(rnn_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model,input_string,seq_length,batch_size):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    if(len(input_string) != seq_length):\n",
    "        raise ValueError(\"input string should be equal to sequence length\")\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size, ctx=context)\n",
    "    sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T\n",
    "                                ,ctx=context)\n",
    "    output,hidden = model(sample_input,hidden)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    index = index.asnumpy()\n",
    "    return [indices_char[char] for char in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapInput(input_str,output_str):\n",
    "    for i,_ in enumerate(input_str):\n",
    "        partial_input = input_str[:i+1]\n",
    "        partial_output = output_str[i:i+1]\n",
    "        print(partial_input + \"->\" + partial_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "p->e\n",
      "pr->e\n",
      "pro->n\n",
      "prob->e\n",
      "proba->n\n",
      "probab->e\n",
      "probabl-> \n",
      "probably-> \n",
      "probably ->a\n",
      "probably t->h\n",
      "probably th->e\n",
      "probably the-> \n",
      "probably the ->t\n",
      "probably the t->h\n",
      "probably the ti->n\n",
      "probably the tim->e\n",
      "probably the time-> \n",
      "probably the time ->t\n",
      "probably the time i->n\n",
      "probably the time is-> \n",
      "probably the time is ->t\n",
      "probably the time is a->n\n",
      "probably the time is at-> \n",
      "probably the time is at ->t\n",
      "probably the time is at h->e\n",
      "probably the time is at ha->n\n",
      "probably the time is at han-> \n",
      "probably the time is at hand-> \n",
      "probably the time is at hand ->a\n",
      "probably the time is at hand w->e\n",
      "probably the time is at hand wh->e\n",
      "probably the time is at hand whe-> \n",
      "probably the time is at hand when-> \n",
      "probably the time is at hand when ->t\n",
      "probably the time is at hand when i->n\n",
      "probably the time is at hand when it-> \n",
      "probably the time is at hand when it ->t\n",
      "probably the time is at hand when it w->e\n",
      "probably the time is at hand when it wi->n\n",
      "probably the time is at hand when it wil-> \n",
      "probably the time is at hand when it will-> \n",
      "probably the time is at hand when it will ->a\n",
      "probably the time is at hand when it will b->e\n",
      "probably the time is at hand when it will be-> \n",
      "probably the time is at hand when it will be ->t\n",
      "probably the time is at hand when it will be o->n\n",
      "probably the time is at hand when it will be on-> \n",
      "probably the time is at hand when it will be onc->e\n",
      "probably the time is at hand when it will be once-> \n",
      "probably the time is at hand when it will be once ->t\n",
      "probably the time is at hand when it will be once a->n\n",
      "probably the time is at hand when it will be once an-> \n",
      "probably the time is at hand when it will be once and-> \n",
      "probably the time is at hand when it will be once and ->a\n",
      "probably the time is at hand when it will be once and a->n\n",
      "probably the time is at hand when it will be once and ag-> \n",
      "probably the time is at hand when it will be once and aga->n\n",
      "probably the time is at hand when it will be once and agai->n\n",
      "probably the time is at hand when it will be once and again-> \n",
      "probably the time is at hand when it will be once and again ->t\n",
      "probably the time is at hand when it will be once and again u->n\n",
      "probably the time is at hand when it will be once and again un-> \n",
      "probably the time is at hand when it will be once and again und-> \n",
      "probably the time is at hand when it will be once and again unde-> \n",
      "probably the time is at hand when it will be once and again under-> \n",
      "probably the time is at hand when it will be once and again unders-> \n",
      "probably the time is at hand when it will be once and again underst->e\n",
      "probably the time is at hand when it will be once and again understo->n\n",
      "probably the time is at hand when it will be once and again understoo->n\n",
      "probably the time is at hand when it will be once and again understood-> \n",
      "probably the time is at hand when it will be once and again understood ->t\n",
      "probably the time is at hand when it will be once and again understood W->o\n",
      "probably the time is at hand when it will be once and again understood WH-> \n",
      "probably the time is at hand when it will be once and again understood WHA->s\n",
      "probably the time is at hand when it will be once and again understood WHAT-> \n",
      "probably the time is at hand when it will be once and again understood WHAT ->a\n",
      "probably the time is at hand when it will be once and again understood WHAT h->e\n",
      "probably the time is at hand when it will be once and again understood WHAT ha->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has ->t\n",
      "probably the time is at hand when it will be once and again understood WHAT has a->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has ac->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has act->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has actu->t\n",
      "probably the time is at hand when it will be once and again understood WHAT has actua->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has actual-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actuall-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually ->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually s->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually su->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suf-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suff-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffi->t\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffic->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffice-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed ->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed a->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed an-> \n"
     ]
    }
   ],
   "source": [
    "test_input = 'probably the time is at hand when it will be once and again understood WHAT has actually sufficed an'\n",
    "print(len(test_input))\n",
    "result= evaluate_seq2seq(model,test_input,seq_length,1)\n",
    "mapInput(test_input,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def generate_random_text(model,input_string,seq_length,batch_size,sentence_length):\n",
    "    count = 0\n",
    "    new_string = ''\n",
    "    cp_input_string = input_string\n",
    "    while count < sentence_length:\n",
    "        idx = [char_indices[c] for c in input_string]\n",
    "        if(len(input_string) != seq_length):\n",
    "            raise ValueError('the was a error in the input ')\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size, ctx=context)\n",
    "        sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T\n",
    "                                ,ctx=context)\n",
    "        output,hidden = model(sample_input,hidden)\n",
    "        index = mx.nd.argmax(output, axis=1)\n",
    "        index = index.asnumpy()\n",
    "        count = count + 1\n",
    "        new_string = new_string + indices_char[index[-1]]\n",
    "        input_string = input_string[1:] + indices_char[index[-1]]\n",
    "    print(cp_input_string + new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_random_text(model,\"probably the time is at hand when it will be once and again understood WHAT has actually sufficed\",seq_length,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
